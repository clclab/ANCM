{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d6f8aac-7388-4e57-b892-ab9ba9407f60",
      "metadata": {
        "id": "0d6f8aac-7388-4e57-b892-ab9ba9407f60"
      },
      "source": [
        "# Modeling Language Processing: Large Language Models & their use in Predicting eye tracking features\n",
        "\n",
        "In this tutorial we will be using language models to predict eye movements during natural reading of English sentences. This notebook is partly based on a tutorial by Nora Hollenstein taught during this year's [ESSLLI course](https://esslli2021.unibz.it/page/course/analyzing_the_cognitive_plausibility_of_deep_language_models/) on cognitive plausibility of language models. It uses eye tracking data from the [Zurich Cognitive Language Processing Corpus (ZuCo)](https://www.nature.com/articles/sdata2018291), as provided for the [CMCL 2021 Shared Task](https://cmclorg.github.io/shared_task). This tutorial was adjusted and extended for ICCN'21 by Marianne de Heer Kloots and Jelle Zuidema.\n",
        "\n",
        "Eye-tracking data from reading represent an important resource for both linguistics and natural language processing. The ability to accurately model gaze features is a key source of information for advancing our understanding of language processing. On one hand, it can tell us how well our computational models align with human language processing mechanisms. On the other hand, it may provide more accurate models of reading to further psycholinguistic research.\n",
        "\n",
        "**What you'll learn**: At the end of this tutorial, you will...\n",
        "- have analyzed transformer-based large language models, and computed and visualized both static and contextualized embeddings\n",
        "- be acquainted with eye tracking data and the structure of the ZuCo corpus\n",
        "- have used transformer language models to predict eye tracking features recorded during reading (fixation duration, fixation proportion, etc.)\n",
        "- have compared the performance of a pre-trained model and a model fine-tuned on eye tracking data\n",
        "- evaluate and reflect on the ability of a contextualized language model to predict psycholinguistic data.\n",
        "\n",
        "Throughout the tutoroial there are exercises for you to do. For some excercises you'll receive points, which will determine your grade (**10 points in total, 10% of the grade for the entire course**).\n",
        "\n",
        "## Contents\n",
        "1. Setup\n",
        "2. Exploring the ZuCo corpus\n",
        "3. Visualizing eye tracking features\n",
        "3. Pre-trained language models\n",
        "4. Finetuning the language model on eye-tracking data\n",
        "5. Evaluating model predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hu-unwAE5_Mk",
      "metadata": {
        "id": "Hu-unwAE5_Mk"
      },
      "source": [
        "![ICCN-CCN-figure](https://github.com/clclab/ANCM/blob/main/book/img/ICCN-CCN-figure.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LLHS_iYQ75rL",
      "metadata": {
        "id": "LLHS_iYQ75rL"
      },
      "source": [
        "In this tutorial we will look at Large Language Models from Artificial Intelligence, and eye-tracking data from cognitive science, and consider how well the models help understand the data. We will not look at neuroscience data today, but the same models and some logic is used as in the work we discussed in the lecture and journal club where fMRI, ECoG, EEG and MEG data were considered."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3YJ6kXh86Ca6",
      "metadata": {
        "id": "3YJ6kXh86Ca6"
      },
      "source": [
        "# Exercises (<font color=\"darkorange\"><b>ToDos</b></font>/<font color=\"cornflowerblue\"><b>ToThinks</b></font>)\n",
        "We believe that the best way to understand analysis methods is by *actually programming* the analyses yourself. You have already seen some of these (ungraded) exercises, which we call “ToDos”. There are different types of exercises throughout the notebook, including:\n",
        "- <font color=\"darkorange\"><b>ToDos</b></font> : short programming exercises\n",
        "- <font color=\"cornflowerblue\"><b>ToThinks</b></font>: questions about the (preceding) text/material\n",
        "Sometimes, you also encounter <font color=\"limegreen\"><b>Tips and Tricks</b></font>, which may contain advice, more information on a specific topic, or links to relevant websites or material.\n",
        "For the <font color=\"cornflowerblue\"><b>ToThinks</b></font>, you’ll receive points, which will determine your grade (**10 points in total**). The number of points that can be earned, will be specified in the cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f72488b-49f9-42d0-830c-79103af2b77e",
      "metadata": {
        "id": "7f72488b-49f9-42d0-830c-79103af2b77e"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O8CvWj45iYSf",
      "metadata": {
        "id": "O8CvWj45iYSf"
      },
      "source": [
        "## Preparation I: Packages, data and helper functions\n",
        "Run these cells to download and import the required packages for this notebook, the eye-tracking dataset, and some helper functions that we will need later for training and evaluating models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf855db-6cbe-4f16-af67-2663407b5623",
      "metadata": {
        "id": "bdf855db-6cbe-4f16-af67-2663407b5623",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install and import required packages in the current Jupyter kernel\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q numpy pandas seaborn matplotlib torch transformers spacy scipy\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import cosine, euclidean, pdist, squareform, is_valid_dm, cdist\n",
        "import seaborn as sns\n",
        "\n",
        "# load English SpaCy model\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# making sure Spacy doesn't mess with our tokenization\n",
        "from spacy.tokens import Doc\n",
        "def custom_tokenizer(wordlist):\n",
        "    \"\"\"replace spacy tokenizer with already tokenized list of words\"\"\"\n",
        "    return Doc(nlp.vocab, words=wordlist, spaces=None)\n",
        "nlp.tokenizer = custom_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QlH15zZhCH_r",
      "metadata": {
        "id": "QlH15zZhCH_r",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download the eyetracking data\n",
        "\n",
        "# create folder to store data\n",
        "if os.getcwd()[-1] == '4':\n",
        "  print('Data already downloaded!')\n",
        "else:\n",
        "  try:\n",
        "    os.mkdir('week4')\n",
        "  except:\n",
        "    print('Main folder already exists')\n",
        "  os.chdir('week4')\n",
        "  try:\n",
        "    os.mkdir('data')\n",
        "  except:\n",
        "    print('Data folder already exists')\n",
        "  try:\n",
        "    os.mkdir('models')\n",
        "  except:\n",
        "    print('Model folder already exists')\n",
        "\n",
        "  # download files\n",
        "  !wget -q --no-check-certificate https://raw.githubusercontent.com/clclab/ICCN-Language/main/model.py -O model.py\n",
        "  !wget -q --no-check-certificate https://raw.githubusercontent.com/clclab/ICCN-Language/main/dataloader.py -O dataloader.py\n",
        "  !wget -q --no-check-certificate https://raw.githubusercontent.com/clclab/ICCN-Language/main/evaluation_metric.py -O evaluation_metric.py\n",
        "  !wget -q --no-check-certificate https://raw.githubusercontent.com/clclab/ICCN-Language/main/LM_functions.py -O LM_functions.py\n",
        "\n",
        "  !wget -q --no-check-certificate https://raw.githubusercontent.com/clclab/ICCN-Language/main/data/README.md -O data/README.md\n",
        "  !wget -q --no-check-certificate https://raw.githubusercontent.com/clclab/ICCN-Language/main/data/test_data.csv -O data/test_data.csv\n",
        "  !wget -q --no-check-certificate https://raw.githubusercontent.com/clclab/ICCN-Language/main/data/training_data.csv -O data/training_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S6tBNVml5RMd",
      "metadata": {
        "id": "S6tBNVml5RMd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Importing helper functions\n",
        "from dataloader import *\n",
        "from model import *\n",
        "from evaluation_metric import *\n",
        "from LM_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_7oG1K5mEyTc",
      "metadata": {
        "id": "_7oG1K5mEyTc"
      },
      "source": [
        "## <b> </b>\n",
        "\n",
        "If you now go to the File tab (red arrow in the image below) you should see a folder named _week4_, which contains a folder named _data_ with the downloaded files (and also a folder named _models_, and some other files)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l7vFgCNWxxom",
      "metadata": {
        "id": "l7vFgCNWxxom"
      },
      "source": [
        "![ICCN-file-folders](https://github.com/clclab/ANCM/blob/main/book/img/ICCN-file-folders.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "luC63qzlsY-n",
      "metadata": {
        "id": "luC63qzlsY-n"
      },
      "source": [
        "## Preparation II: downloading the large language models\n",
        "\n",
        "Next, we're going to download the language models that we will use in this tutorial. We will load these into the colab environment from Google Drive, similar to how you've loaded the EEG data in week 2. For this, you need to do the following:\n",
        "\n",
        "**1. Give Colab permission to access Google Drive** Execute the cell below, follow the instructions and copy the token. You now should be able to access your drive through the File tab via the folder *drive*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LxqdMqjYs8Su",
      "metadata": {
        "id": "LxqdMqjYs8Su"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50NIuXyutDQV",
      "metadata": {
        "id": "50NIuXyutDQV"
      },
      "source": [
        "**2. Add the model files to Google Drive** Below you see how to add the files to My Drive. Please read the instructions carefully and follow the steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JlUOsA6wzMQz",
      "metadata": {
        "id": "JlUOsA6wzMQz"
      },
      "source": [
        "\n",
        "![ICCN-drive-steps](https://github.com/clclab/ANCM/blob/main/book/img/ICCN-drive-steps.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "liuVpvMBtROT",
      "metadata": {
        "id": "liuVpvMBtROT"
      },
      "source": [
        "1. The links below will give you access to the models. DON'T click on the blue button, but instead, click on the icon on top for adding a shortcut to your drive (see above). Alternatively, you may need to click the triple points ($\\vdots$) in the top right, and click on the option 'Organize'.\n",
        "  - [distilbert-base-uncased_0](https://drive.google.com/file/d/1DxfBsCfxRhEPaKfpiWpBFjuugGWpmTQl/view?usp=sharing)\n",
        "  - [distilbert-base-uncased_150](https://drive.google.com/file/d/1AVfYpZEkf4y4uonFBoIJrPM6K4FsTayF/view?usp=sharing)\n",
        "2. A dropdown menu will appear, where you should click the button to add the shortcut to your drive. If you want, you can choose or create a subfolder within your Google Drive to store the models.\n",
        "3. You should then get a confirmation that the shortcut is stored to your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ly9XgZJL0qYt",
      "metadata": {
        "id": "Ly9XgZJL0qYt"
      },
      "source": [
        "**3. Set directory** To import the models in this notebook, you need to specify where in your drive the models are stored. If you saved the files in My Drive you can execute the cell below immediately. (If you've saved it in another folder, then change the path accordingly.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rgA0C2wp0r3-",
      "metadata": {
        "id": "rgA0C2wp0r3-"
      },
      "outputs": [],
      "source": [
        "distilbert_0_file = '/content/drive/MyDrive/distilbert-base-uncased_0'\n",
        "distilbert_150_file = '/content/drive/MyDrive/distilbert-base-uncased_150'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BD6Ou-t2FJnt",
      "metadata": {
        "id": "BD6Ou-t2FJnt"
      },
      "source": [
        "# Data: Exploring the ZuCo corpus\n",
        "The cell below loads the ZuCo corpus and prints the number of sentences in the training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf2352c-4860-4d8f-ba04-11b9ecf033e3",
      "metadata": {
        "id": "1cf2352c-4860-4d8f-ba04-11b9ecf033e3"
      },
      "outputs": [],
      "source": [
        "training_data = pd.read_csv(\"data/training_data.csv\")\n",
        "test_data = pd.read_csv(\"data/test_data.csv\")\n",
        "\n",
        "print(len(training_data['sentence_id'].unique()), \"training sentences including\", len(training_data), \"words.\")\n",
        "print(len(test_data['sentence_id'].unique()), \"test sentences including\", len(test_data), \"words.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bhQo6MFLUul",
      "metadata": {
        "cellView": "form",
        "id": "1bhQo6MFLUul"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 1</b></font> (1pt)\n",
        "#@markdown The data are randomly split into a larger train set and a smaller test set. Both parts are thus assumed to be independent samples from the same distribution. We will train our models on the train set, and evaluate them on the test set. Can we repeat this train & test cycle multiple times? Why? (max 5 sentences).\n",
        "\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e4ff01-2068-41f2-929e-331964411a48",
      "metadata": {
        "id": "46e4ff01-2068-41f2-929e-331964411a48"
      },
      "source": [
        "### Eye tracking features explained\n",
        "\n",
        "For each word, there are five eye tracking features:\n",
        "- _nFix_ (number of fixations): total number of fixations on the current word.\n",
        "- _FFD_ (first fixation duration): the duration of the first fixation on the prevailing word.\n",
        "- _GPT_ (go-past time): the sum of all fixations prior to progressing to the right of the current word, including regressions to previous words that originated from the current word.\n",
        "- _TRT_ (total reading time): the sum of all fixation durations on the current word, including regressions.\n",
        "- _fixProp_ (fixation proportion): the proportion of participants that fixated the current word (as a proxy for how likely a word is to be fixated).\n",
        "\n",
        "<font color='limegreen'><b>Tip</b></font> If you are unfamiliar with eye tracking data, have a look at [this website](https://www.sr-research.com/eye-tracking-blog/background/eye-tracking-terminology-eye-movements/) to find out what a fixation is."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xmn9ox-ZFzA-",
      "metadata": {
        "id": "Xmn9ox-ZFzA-"
      },
      "source": [
        "Originally, the ZuCo corpus contains eye tracking data from 30 participants. The data was randomly shuffled before splitting into training and test data. Here, the data has already been averaged across readers, and the feature values have been _scaled to a range between 0 and 100_. The eye-tracking feature values are scaled to facilitate evaluation via the mean absolute error. The features nFix and fixProp are scaled separately, while FFD, GPT and TRT are scaled together since these are all dependent and measured in milliseconds.\n",
        "\n",
        "<font color='darkorange'><b>ToDo</b></font> Let's have a look at some sentences and their eye tracking features to get a better idea of the data. Change the sentence_id in the cell below to see a few other sentences (0 - 799)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uV1mVxUKFn28",
      "metadata": {
        "id": "uV1mVxUKFn28"
      },
      "outputs": [],
      "source": [
        "training_data.loc[training_data['sentence_id'] == 444]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CqkGrfgFHeRp",
      "metadata": {
        "cellView": "form",
        "id": "CqkGrfgFHeRp"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 2</b></font> (0.5pt)\n",
        "#@markdown Why are the values in the GPT column always at least as high as those in the FFD column?\n",
        "\n",
        "#As described above, some preprocessing was performed to get to the numbers in the table above. Make sure you understand the part about feature scaling. What does it mean for interpreting, for example, the values of the variables originally measured in milliseconds?\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b5053b-b64c-4fa3-812e-3e3abfa3182b",
      "metadata": {
        "id": "87b5053b-b64c-4fa3-812e-3e3abfa3182b"
      },
      "source": [
        "### A note on tokenization\n",
        "In NLP, tokenization is the task of breaking down text into smaller units called tokens. Tokens can be words, parts of words, or even just characters like punctuation.\n",
        "\n",
        "The tokens in this dataset are split in the same manner as they were presented to the participants during the eye tracking experiments. For example, the sequences\n",
        "`(except,` and `don't` were presented as such to the reader and not split into `(`, `except`, `,` and `do`, `n't` as might be more useful when presenting text to a computer.\n",
        "Sentence endings are marked with an `<EOS>` symbol added to the last token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rngp71AjQLd9",
      "metadata": {
        "id": "rngp71AjQLd9"
      },
      "source": [
        "# Visualizing eye tracking features\n",
        "\n",
        "<font color='darkorange'><b>ToDo</b></font> Try out the two visualizations below (the bubble and bar plots) for a few different sentences, by changing the sentence_id in the cell below and rerunning the plot cells afterwards. Also try visualizing a few different features by changing the feature_to_draw variable in the first line of each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Bzpa72lWRP7",
      "metadata": {
        "id": "_Bzpa72lWRP7"
      },
      "outputs": [],
      "source": [
        "example_sentence = training_data.loc[training_data['sentence_id'] == 444]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2DMoaHYbUkME",
      "metadata": {
        "id": "2DMoaHYbUkME"
      },
      "source": [
        "## Visualizing fixation times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b418433-5957-4935-86e9-1530bcf14223",
      "metadata": {
        "id": "6b418433-5957-4935-86e9-1530bcf14223"
      },
      "outputs": [],
      "source": [
        "feature_to_draw = \"TRT\" # or \"GPT\" or \"TRT\"\n",
        "\n",
        "fig = plt.figure(figsize=(len(example_sentence)+2,3))\n",
        "words = [s.replace(\"<EOS>\", \"\") for s in example_sentence[\"word\"]]\n",
        "ax = sns.scatterplot(data=example_sentence, x=\"word_id\", y=0, size=feature_to_draw, hue=feature_to_draw, palette=\"flare\", sizes=(200,5000), legend=False)\n",
        "ax.set_frame_on(False)\n",
        "ax.tick_params(axis=u'both', which=u'both',length=0)\n",
        "plt.xticks(ticks=example_sentence[\"word_id\"], labels=words, fontsize=14)\n",
        "plt.yticks([0], [feature_to_draw], fontsize=14)\n",
        "plt.ylim(-0.05,0.07)\n",
        "plt.xlabel(None)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fRapGvCVV009",
      "metadata": {
        "id": "fRapGvCVV009"
      },
      "source": [
        "## Visualizing fixation proportion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e868ae25-b2d6-449d-a23c-04391222fce8",
      "metadata": {
        "id": "e868ae25-b2d6-449d-a23c-04391222fce8"
      },
      "outputs": [],
      "source": [
        "feature_to_draw = \"fixProp\" # or nFix\n",
        "\n",
        "fig = plt.figure(figsize=(len(example_sentence)+2,3))\n",
        "words = [s.replace(\"<EOS>\", \"\") for s in example_sentence[\"word\"]]\n",
        "ax = sns.barplot(data=example_sentence, x=\"word_id\", y=feature_to_draw, color=\"lightblue\")\n",
        "ax.set_frame_on(False)\n",
        "ax.tick_params(axis=u'both', which=u'both',length=0)\n",
        "plt.xticks(ticks=example_sentence[\"word_id\"], labels=words, fontsize=14)\n",
        "plt.xlabel(None)\n",
        "plt.ylabel(feature_to_draw, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ly0Ms5X_Yrpl",
      "metadata": {
        "cellView": "form",
        "id": "Ly0Ms5X_Yrpl"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 3</b></font> (1.5pt)\n",
        "#@markdown What kind of properties of words might cause longer fixation times? Think of at least two properties (1 pt). Can you already spot such a relationship between particular kinds of words and eye tracking features in the example sentences you explored? Include a sentence_id, eye tracking feature and some words to illustrate your answer (0.5 pt).\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FwZ2Ka8xYF06",
      "metadata": {
        "id": "FwZ2Ka8xYF06"
      },
      "source": [
        "## Training data overview\n",
        "\n",
        "Now let's get a complete overview of all eye tracking feature values in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92bc50e1-55ff-4db9-a514-bb99d5ab0f3d",
      "metadata": {
        "id": "92bc50e1-55ff-4db9-a514-bb99d5ab0f3d"
      },
      "outputs": [],
      "source": [
        "c = sns.color_palette(\"hls\", 10)\n",
        "sns.set(font_scale = 1)\n",
        "sns.set_style(\"whitegrid\")\n",
        "ax = sns.boxplot(data=training_data[[\"nFix\",\"FFD\",\"GPT\",\"TRT\",\"fixProp\"]], palette=c[5:], color='grey', linewidth=1, fliersize=1)\n",
        "\n",
        "medians = []\n",
        "for f in [\"nFix\",\"FFD\",\"GPT\",\"TRT\",\"fixProp\"]:\n",
        "    median = training_data[f].median()\n",
        "    medians.append(median)\n",
        "median_labels = [str(np.round(s, 2)) for s in medians]\n",
        "\n",
        "pos = range(len(medians))\n",
        "for tick,label in zip(pos,ax.get_xticklabels()):\n",
        "    ax.text(pos[tick], -4, median_labels[tick], #medians[tick] + offsets[tick]\n",
        "            horizontalalignment='center', size='small', color='black')#, weight='semibold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01e232e-e3c3-40d1-b082-4d750984827f",
      "metadata": {
        "id": "c01e232e-e3c3-40d1-b082-4d750984827f"
      },
      "source": [
        "# Models: using pre-trained large language models\n",
        "\n",
        "In this tutorial we will work with a variant of BERT, a state-of-the-art language model trained for English. BERT was the first widely successful transformer-based language model and remains highly influential. DistilBERT is a variant of BERT that requires less training time due to a considerable reduction of the training parameters while maintaining similar performance on benchmark datasets. In the end, we will finetune the model so that we can use it for predicting eye-tracking features from text. But let's first look at the insides of DistilBERT itself.\n",
        "\n",
        "Note: BERT and DistilBERT are so-called 'masked language model'. Classical language modelling is about computing the probabilities of possible next words given *prior* words. Masked language modelling is also about predicting a word given prior words, but additionally a masked language model may also use information about *future* words. E.g., the model is given a sentence like \"The dog [MASK] loudly\", an can predict \"barked\" or another word at the masked position, based on both \"the dog\" and \"loudly\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eKW-w2XS1aRw",
      "metadata": {
        "id": "eKW-w2XS1aRw"
      },
      "source": [
        "### Word embeddings with pen & paper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efyDQp5uv7V_",
      "metadata": {
        "id": "efyDQp5uv7V_"
      },
      "source": [
        "Before running the pretrained language models that we downloaded, we will first consider briefly what word embeddings are and how they can represent useful information about the meaning (and grammatical properties) of words.\n",
        "\n",
        "Consider the following words: child, son, daughter, parent, father, mother, king, queen, princess, prince.\n",
        "\n",
        "<font color='darkorange'><b>ToDo</b></font> With pen & paper (!), draw a 3 dimensional 'semantic space' that could represent the distinctions between these words. Position each of the words in the space (assuming a word like 'father' is, most of the time 'non-royal')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2ktqmqn0YSX",
      "metadata": {
        "cellView": "form",
        "id": "m2ktqmqn0YSX"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 4</b></font> (1pt)\n",
        "#@markdown What are the length-3 word embeddings for each of these words? What is the (Euclidean) distance between 'father' and 'princess'?. What's PRINCE+(MOTHER-FATHER)? What's PRINCE+(MOTHER-FATHER)+(PARENT-CHILD)?\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JUrjmTaN0RHA",
      "metadata": {
        "id": "JUrjmTaN0RHA"
      },
      "source": [
        "## Word embeddings from a large language model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h9gG_rMl48yo",
      "metadata": {
        "id": "h9gG_rMl48yo"
      },
      "source": [
        "Make the 'distilbert' language model and the package scprep available, and make a list of the words. (We will call that list \"sentences\" as we will later look at sentences of more than 1 word)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lzP_nW1zJLrj",
      "metadata": {
        "id": "lzP_nW1zJLrj"
      },
      "outputs": [],
      "source": [
        "# @title Download DistilBERT model and tokenizer from HuggingFace\n",
        "model_name = 'distilbert-base-uncased'\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rXBf-E6Y5s0U",
      "metadata": {
        "id": "rXBf-E6Y5s0U"
      },
      "outputs": [],
      "source": [
        "!pip install scprep #phate umap-learn\n",
        "import scprep\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w9NvNzfu5fSa",
      "metadata": {
        "id": "w9NvNzfu5fSa"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "         \"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"prince\", \"princess\"\n",
        "#\"mouse\", \"rat\", \"elephant\", \"mosquito\", \"crocodile\", \"pigeon\", \"dog\", \"cat\", \"rabbit\", \"squirl\", \"cockroach\", \"democracy\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifE0nxI6pUA",
      "metadata": {
        "id": "fifE0nxI6pUA"
      },
      "source": [
        "The next step is present each of the 'sentences' to the language model, and retrieve the activation vectors at each layer. The following code does that for you, and puts the input embeddings in the first element of the list *vecs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CQDW1vm16dt0",
      "metadata": {
        "id": "CQDW1vm16dt0"
      },
      "outputs": [],
      "source": [
        "#@title Retrieve activation vectors from various layers (stored in *vecs*)\n",
        "from collections import defaultdict\n",
        "vecs = []\n",
        "layer_result = defaultdict(list)\n",
        "sent_words = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded = tokenizer(sent, return_tensors=\"pt\")\n",
        "    inputs = encoded.input_ids\n",
        "    attention_mask =  encoded['attention_mask']\n",
        "    output = model(input_ids=inputs, attention_mask=attention_mask)\n",
        "    states = output.hidden_states\n",
        "    token_len = attention_mask.sum().item()\n",
        "    decoded = tokenizer.convert_ids_to_tokens(inputs[0], skip_special_tokens=False)\n",
        "    if model_name in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]:\n",
        "        word_indices = np.array(list(map(lambda e: -1 if e is None else e, encoded.word_ids())))[:token_len]\n",
        "        word_groups = np.split(np.arange(word_indices.shape[0]), np.unique(word_indices, return_index=True)[1])[1:]\n",
        "        sw = [\"\".join(list(map(lambda t: t[1:] if t[:1] == \"Ġ\" else t, np.array(decoded)[g]))) for g in word_groups]\n",
        "        sent_words.append(sw)\n",
        "    else:\n",
        "        word_indices = np.array(list(map(lambda e: -1 if e is None else e, encoded.word_ids())))[1:token_len - 1]\n",
        "        word_groups = np.split(np.arange(word_indices.shape[0]) + 1, np.unique(word_indices, return_index=True)[1])[1:]\n",
        "        sent_words.append([\"\".join(list(map(lambda t: t[2:] if t[:2] == \"##\" else t, np.array(decoded)[g]))) for g in word_groups])\n",
        "\n",
        "    for n, layer_group in enumerate([[0], [1], [2], [3], [4], [5], [6]]):\n",
        "        sent_vec = combine_output_for_layers(model, inputs, states, word_groups, layer_group)\n",
        "        layer_result[n].append(sent_vec)\n",
        "\n",
        "vecs = [np.concatenate(r) for r in layer_result.values()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XXgrWd2e7fUE",
      "metadata": {
        "id": "XXgrWd2e7fUE"
      },
      "source": [
        "Now have a look at the learned input embeddings of the model. It's useful to import them in a DataFrame for nicer viewing and compatibility with statistics packages.\n",
        "\n",
        "<font color='darkorange'><b>ToDo</b></font> enter the following code in a code cell and check in the table what the columns and rows mean.\n",
        "\n",
        "staticembeddings = pd.DataFrame(vecs[0])\n",
        "\n",
        "print(staticembeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q4C03yXY9QOR",
      "metadata": {
        "id": "q4C03yXY9QOR"
      },
      "source": [
        "We can try to visualize the high-dimensional static embeddings by running a standard dimensionality reduction technique, PCA, on them and plotting the position of each word in space consisting of the first three principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4RQTQto283j3",
      "metadata": {
        "id": "4RQTQto283j3"
      },
      "outputs": [],
      "source": [
        "staticembeddings = pd.DataFrame(vecs[0])\n",
        "\n",
        "print(staticembeddings)\n",
        "data_pca = scprep.reduce.pca(staticembeddings, n_components=3, method='dense')\n",
        "scprep.plot.scatter3d(data_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9s4nIfp57aIJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "9s4nIfp57aIJ",
        "outputId": "25170900-6e53-4ea3-95a0-d91a1dfe5ddd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f7da795b-7169-4f19-a57b-5e3698b23cff\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f7da795b-7169-4f19-a57b-5e3698b23cff\")) {                    Plotly.newPlot(                        \"f7da795b-7169-4f19-a57b-5e3698b23cff\",                        [{\"hovertemplate\":\"index=boy\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"boy\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"boy\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.4639016091823578],\"y\":[-0.28861114382743835],\"z\":[-0.3896322548389435],\"type\":\"scatter3d\"},{\"hovertemplate\":\"index=girl\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"girl\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"girl\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.43047624826431274],\"y\":[-0.27368810772895813],\"z\":[0.15085038542747498],\"type\":\"scatter3d\"},{\"hovertemplate\":\"index=man\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"man\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"man\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.525477945804596],\"y\":[0.35772451758384705],\"z\":[-0.15198160707950592],\"type\":\"scatter3d\"},{\"hovertemplate\":\"index=woman\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"woman\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"woman\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.45454251766204834],\"y\":[0.12494822591543198],\"z\":[0.4330417811870575],\"type\":\"scatter3d\"},{\"hovertemplate\":\"index=king\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"king\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"king\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.38922396302223206],\"y\":[0.5471477508544922],\"z\":[-0.2485445886850357],\"type\":\"scatter3d\"},{\"hovertemplate\":\"index=queen\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"queen\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"queen\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.5079616904258728],\"y\":[0.23836471140384674],\"z\":[0.3733002245426178],\"type\":\"scatter3d\"},{\"hovertemplate\":\"index=prince\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"prince\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"prince\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.5040795803070068],\"y\":[-0.2261345386505127],\"z\":[-0.4101938009262085],\"type\":\"scatter3d\"},{\"hovertemplate\":\"index=princess\\u003cbr\\u003ePC1=%{x}\\u003cbr\\u003ePC2=%{y}\\u003cbr\\u003ePC3=%{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"princess\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"princess\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[-0.4731331765651703],\"y\":[-0.4797516465187073],\"z\":[0.24315984547138214],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"PC1\"}},\"yaxis\":{\"title\":{\"text\":\"PC2\"}},\"zaxis\":{\"title\":{\"text\":\"PC3\"}}},\"legend\":{\"title\":{\"text\":\"index\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f7da795b-7169-4f19-a57b-5e3698b23cff');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# create a dataframe with the words as index\n",
        "staticembeddings = pd.DataFrame(vecs[0], index = sentences)\n",
        "\n",
        "#compute pca\n",
        "data_pca = scprep.reduce.pca(staticembeddings, n_components=3, method='dense')\n",
        "\n",
        "# plot using plotly an interactive plotting library\n",
        "import plotly.express as px\n",
        "fig = px.scatter_3d(data_pca, x='PC1', y='PC2', z='PC3', color=data_pca.index)\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8SlEVvKW-Apw",
      "metadata": {
        "id": "8SlEVvKW-Apw"
      },
      "source": [
        "<font color='darkorange'><b>ToDo</b></font> Inspect the PCA vectors and check where the words are positioned in this 3-dimensional space. Compare the space with the space you drew in the pen & paper exercise above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TUL3cGi_AFWG",
      "metadata": {
        "id": "TUL3cGi_AFWG"
      },
      "source": [
        "<font color='darkorange'><b>ToDo</b></font> Think of another collection of words that might be interesting to inspect in this way and see if you can plot an interpretable 3-dimensional word space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'><b>Extra</b></font> When you compare the word vector positions in the PCA to your pen&paper drawing, you probably identified a good match between both. However, be careful in generalizing this observation as the way the DistilBert embeddings were plotted might have simply been coincidence!\n",
        "\n",
        "Run the following code block with PCA based on the static word embeddings generated with another popular model, namely RoBERTa. What do you notice when comparing both PCAs (DistilBert & RoBERTa vectors) to your pen&paper results? What can you infer from this? (This is not a graded exercise.)"
      ],
      "metadata": {
        "id": "sqSK83xff_io"
      },
      "id": "sqSK83xff_io"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download RoBERTa model and tokenizer from HuggingFace & plot new PCA\n",
        "model_name = 'roberta-base'\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# @title Retrieve activation vectors from all layers (stored in *vecs*)\n",
        "from collections import defaultdict\n",
        "vecs = []\n",
        "layer_result = defaultdict(list)\n",
        "sent_words = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded = tokenizer(sent, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = encoded.input_ids\n",
        "    attention_mask = encoded['attention_mask']\n",
        "    output = model(input_ids=inputs, attention_mask=attention_mask)\n",
        "    states = output.hidden_states  # All 12 layers\n",
        "    token_len = attention_mask.sum().item()\n",
        "    decoded = tokenizer.convert_ids_to_tokens(inputs[0], skip_special_tokens=False)\n",
        "\n",
        "    word_indices = np.array(list(map(lambda e: -1 if e is None else e, encoded.word_ids())))[1:token_len - 1]\n",
        "    word_groups = np.split(np.arange(word_indices.shape[0]) + 1, np.unique(word_indices, return_index=True)[1])[1:]\n",
        "    sent_words.append([\"\".join(list(map(lambda t: t[2:] if t[:2] == \"##\" else t, np.array(decoded)[g]))) for g in word_groups])\n",
        "\n",
        "    for n, layer_group in enumerate(range(12)):  # all 12 layers\n",
        "        sent_vec = combine_output_for_layers(model, inputs, states, word_groups, [layer_group])\n",
        "        layer_result[n].append(sent_vec)\n",
        "\n",
        "vecs = [np.concatenate(r) for r in layer_result.values()]\n",
        "\n",
        "staticembeddings = pd.DataFrame(vecs[0])\n",
        "\n",
        "print(staticembeddings)\n",
        "\n",
        "data_pca = scprep.reduce.pca(staticembeddings, n_components=3, method='dense')\n",
        "scprep.plot.scatter3d(data_pca)\n",
        "\n",
        "# Create a dataframe with the words as index\n",
        "staticembeddings = pd.DataFrame(vecs[0], index=sentences)\n",
        "\n",
        "# Compute PCA\n",
        "data_pca = scprep.reduce.pca(staticembeddings, n_components=3, method='dense')\n",
        "\n",
        "# Plot using plotly, an interactive plotting library\n",
        "import plotly.express as px\n",
        "fig = px.scatter_3d(data_pca, x='PC1', y='PC2', z='PC3', color=data_pca.index)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "zMPfB7cKhdbt",
        "cellView": "form"
      },
      "id": "zMPfB7cKhdbt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "sKhGJy-BJajA",
      "metadata": {
        "id": "sKhGJy-BJajA"
      },
      "source": [
        "### Static versus contextualized word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-EkvnjldJpvp",
      "metadata": {
        "id": "-EkvnjldJpvp"
      },
      "source": [
        "In Large Language Models, the input embeddings are static (unique for each word/token). But the activation vectors of higher layers in the model are often more useful (e.g., for predicting brain activation). These vectors, however, are not static; if we use them to build up a representation for words, we call these representations \"contextualized embeddings\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ESlYwodrK-TX",
      "metadata": {
        "cellView": "form",
        "id": "ESlYwodrK-TX"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 5</b></font> (1pt)\n",
        "#@markdown Use a pair of sentences in *sentence* with ambiguous words and a disambiguating context, and report the first three numbers of the word embeddings retrieved from layer 0 (static) and from layer 4 (contextualized).\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wjFrJO3mLz_a",
      "metadata": {
        "id": "wjFrJO3mLz_a"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"A computer needs a mouse. A cat eats a mouse.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VMN7kFFJKl4G",
      "metadata": {
        "id": "VMN7kFFJKl4G"
      },
      "source": [
        "We can use tools like \"distance (or dissimilarity) matrices\" and \"representational similarity analysis\" to understand better the structure of the embeddings spaces. Below is some code that will compute dissimilarity matrices for you for embeddings retrieved from the different layers of the Large Language Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cmOvh8n_OSp0",
      "metadata": {
        "id": "cmOvh8n_OSp0"
      },
      "outputs": [],
      "source": [
        "#@title Compute distance matrices between embeddings\n",
        "def compute_distance_matrices(vecs, measure):\n",
        "  distance_matrices = [\n",
        "    cdist(vec, vec, measure).round(10)\n",
        "        for vec in vecs\n",
        "  ]\n",
        "  return distance_matrices\n",
        "\n",
        "# measure can be \"cosine\" or \"euclidean\"\n",
        "distance_matrices = compute_distance_matrices(vecs, measure=\"cosine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KyNaOZcKQWgw",
      "metadata": {
        "id": "KyNaOZcKQWgw"
      },
      "outputs": [],
      "source": [
        "# @title Plot similarity matrices between word embeddings for different layers\n",
        "model_name = 'distilbert-base-uncased' #model_name = 'DistilBERT'\n",
        "\n",
        "labels = [\"A\", \"computer\", \"needs\",\"a\",\"mouse\",\".\",\"A\",\"cat\",\"eats\",\"a\",\"mouse\",\".\"]\n",
        "\n",
        "plot_data = [\n",
        "    (\"xy\", 0, \"input embedding layer\"),\n",
        "    (\"x\", 1, \"layer 1\"),\n",
        "    (\"xy\", 3, \"layer 3\"),\n",
        "    (\"x\", 6, \"layer 6\"),\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18, 24))\n",
        "fig.suptitle(model_name + ': distance matrices between embeddings across layers')\n",
        "\n",
        "for subplot, (label_axes, matrix_index, title) in zip(itertools.chain.from_iterable(axes), plot_data):\n",
        "    heatmap_args = dict(\n",
        "        linewidth=1,\n",
        "        annot = np.array([[f\"{v:.2f}\" if (v == 0 or len(str(v)) > 4)\n",
        "        else f\"{v:.1f}\" for v in r] for r in distance_matrices[matrix_index]]),\n",
        "        annot_kws={\"size\":6.5},\n",
        "        fmt=\"\",\n",
        "        cmap = 'magma_r',\n",
        "        xticklabels=labels,\n",
        "        yticklabels=labels,\n",
        "    )\n",
        "\n",
        "    heatmap = sns.heatmap(distance_matrices[matrix_index], ax=subplot, **heatmap_args)\n",
        "    subplot.set_title(title)\n",
        "    for axis in [x for x in \"xy\" if x not in label_axes]:\n",
        "        getattr(subplot, f\"{axis}axis\").set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this code in case the resulting RDMs are displayed incorrectly\n",
        "image_file = \"RDMPlots.png\"\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename=image_file))\n"
      ],
      "metadata": {
        "id": "23TtgVR7xEHf",
        "cellView": "form"
      },
      "id": "23TtgVR7xEHf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1r8Ofj1Q__S8",
      "metadata": {
        "cellView": "form",
        "id": "1r8Ofj1Q__S8"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 6</b></font> (1pt)\n",
        "#@markdown Report the distances between computer and mouse1, and cat and mouse2 in layer 0 and layer 6. Explain briefly what is happening.\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98pyKWITfFXc",
      "metadata": {
        "id": "98pyKWITfFXc"
      },
      "source": [
        "<font color='darkorange'>**ToDo**</font>\n",
        "\n",
        "Now that we have distance (disimilarity) matrices, we can quite straightforwardly run Representational Similarity Analysis, between layers of the Large Langauge Model. Try the following, starting by a somewhat larger piece of text, rerunning the **\"retrieve activation vectors\"** code above and then computer distance matrices and representational similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oyUIls4nNQww",
      "metadata": {
        "id": "oyUIls4nNQww"
      },
      "outputs": [],
      "source": [
        "# input sentences for the model\n",
        "sentences = [\n",
        "    \"Harry had never believed he would meet a boy he hated more than Dudley, but that was before he met Draco Malfoy.\",\n",
        "    \"Still, first-year Gryffindors only had Potions with the Slytherins, so they didn't have to put up with Malfoy much.\",\n",
        "    \"Or at least, they didn't until they spotted a notice pinned up in the Gryffindor common room that made them all groan.\"\n",
        "           ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZF6DbWvdRoti",
      "metadata": {
        "id": "ZF6DbWvdRoti"
      },
      "outputs": [],
      "source": [
        "model_name = 'distilbert-base-uncased'\n",
        "\n",
        "distance_matrices_cosine = compute_distance_matrices(vecs[:6], measure=\"cosine\")\n",
        "rsa_mat_cosine = RSA_matrix(distance_matrices_cosine, method='pearson')\n",
        "plot_RSA(rsa_mat_cosine, model_name, dist_method=\"cosine\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rBGmqjVkQ3ue",
      "metadata": {
        "cellView": "form",
        "id": "rBGmqjVkQ3ue"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 7</b></font> (1pt)\n",
        "#@markdown Briefly describe the pattern you observe in the representational similarity between layers, and relate it to the layer-wise predictivity plot we saw in the Schrimpf et al. paper.\n",
        "\n",
        "#@markdown (The intended graph is Figure 2C, and the \"relation\" you are asked to describe is not something very precise: just a rough relation of the patterns you observe in the two graphs. Note that your graph uses DistillBERT without brain data, while Schrimpf et al use GPT2 and brain data, so precise correspondence is not expected.)\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PE-yAbVyIN57",
      "metadata": {
        "id": "PE-yAbVyIN57"
      },
      "source": [
        "# Finetuning the language model on eye-tracking data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1118f6d0-4755-4485-9714-2e1196e4c7a7",
      "metadata": {
        "id": "1118f6d0-4755-4485-9714-2e1196e4c7a7"
      },
      "source": [
        "<font color='darkorange'>**ToDo**</font>\n",
        "\n",
        "Try fine-tuning the DistilBERT model on the eye tracking data for a few epochs using the code in the cell below. This can take a while (e.g. 30 minutes for 5 epochs). It's best to start with a low number of epochs (1 or 2). You can ignore the warning about the model checkpoint. After this cell has finished executing, your fine-tuned model should be saved to the _models_ folder within the _week4_ folder (check the File tab).\n",
        "\n",
        "Alternatively, you **may also skip the cell below** and work with the models that you have added to your Google Drive at the start of this tutorial:\n",
        "- A pre-trained, but not fine-tuned DistilBERT model (fine-tuned for 0 epochs), i.e. `distilbert_0_file`\n",
        "- A fine-tuned DistilBERT model (trained for 150 epochs on the eye-tracking), i.e. `distilbert_150_file`\n",
        "\n",
        "(If you choose the latter, proceed to the _Evaluating model predictions_ section below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be33691e-fac9-4106-9b60-9e78e8753a07",
      "metadata": {
        "id": "be33691e-fac9-4106-9b60-9e78e8753a07"
      },
      "outputs": [],
      "source": [
        "# Note:\n",
        "# You could load any other BERT or DistilBERT model version available in the Huggingface library in the same manner.\n",
        "# DistilBERT was chosen for this tutorial since it has less parameters.\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "random.seed(12345)\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# define model\n",
        "model_name = 'distilbert-base-uncased'\n",
        "regr_model = TransformerRegressionModel(model_name).to(device)\n",
        "train_data = EyeTrackingCSV(training_data, model_name=model_name)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "optimizer = torch.optim.AdamW(regr_model.parameters(), lr=5e-5)\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "# train model\n",
        "# set num_epochs to 0 for no fine-tuning\n",
        "num_epochs = 1\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    train(regr_model, train_loader, optimizer, loss)\n",
        "    print(\"Epoch:\", epoch)\n",
        "print(\"Training done.\")\n",
        "\n",
        "# save model\n",
        "output_model = './models/'+model_name+'_'+str(num_epochs)\n",
        "torch.save(regr_model, output_model)\n",
        "print(\"Model saved to\", output_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84aeadfd-1d06-4504-944f-9f236ae32a4d",
      "metadata": {
        "id": "84aeadfd-1d06-4504-944f-9f236ae32a4d"
      },
      "source": [
        "# Evaluating model predictions\n",
        "\n",
        "We will now load a pre-trained model without fine-tuning, and a pre-trained model that is fine-tuned on eye tracking data, and see how well they do at predicting eye tracking features from text. When you run the cells below, you will see tables with the predictions for the different eye tracking features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b852f8d-8b66-482f-9662-4b736cb1a47e",
      "metadata": {
        "id": "0b852f8d-8b66-482f-9662-4b736cb1a47e"
      },
      "outputs": [],
      "source": [
        "# load pretrained model\n",
        "#model_name = 'distilbert_0'\n",
        "#pretrained_model_to_evaluate = distilbert_0_file\n",
        "\n",
        "# if you want to load your own pretrained model (downloaded in the cell above), uncomment the lines below,\n",
        "# change the filepath to the location of your model, and define your model's name (for saving the predictions file)\n",
        "# note that this should be a model that is *not* finetuned (i.e. 0 epochs)\n",
        "pretrained_model_to_evaluate = './models/distilbert-base-uncased_1'\n",
        "model_name = 'distilbert_0'\n",
        "\n",
        "pretrained_model = torch.load(pretrained_model_to_evaluate)\n",
        "predictions_pretrained = predict(pretrained_model, model_name, test_data)\n",
        "predictions_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b974e73-c4d5-4757-94e7-d722846adf92",
      "metadata": {
        "id": "2b974e73-c4d5-4757-94e7-d722846adf92"
      },
      "outputs": [],
      "source": [
        "# load model fine-tuned on eye-tracking data\n",
        "model_name = 'distilbert_150'\n",
        "finetuned_model_to_evaluate = distilbert_150_file\n",
        "\n",
        "# if you want to load a model that you fine-tuned yourself, uncomment the lines below,\n",
        "# change the filepath to the location of your model, and define your model's name (for saving the predictions file)\n",
        "# finetuned_model_to_evaluate = 'models/distilbert-base-uncased_5'\n",
        "# model_name = 'distilbert_5'\n",
        "\n",
        "finetuned_model = torch.load(finetuned_model_to_evaluate)\n",
        "predictions_finetuned = predict(finetuned_model, model_name, test_data)\n",
        "predictions_finetuned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68c66e6-38a9-4b25-97bd-5861a5e807d2",
      "metadata": {
        "id": "d68c66e6-38a9-4b25-97bd-5861a5e807d2"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "We also need a baseline to compare our models' performances against. We will use the mean central tendency as a baseline, i.e. our estimate for each eye tracking feature will be the mean value of that feature in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3580c6c-9179-48a1-8fbf-e123c894dbbd",
      "metadata": {
        "id": "a3580c6c-9179-48a1-8fbf-e123c894dbbd"
      },
      "outputs": [],
      "source": [
        "# compute mean values from training file\n",
        "features = [\"nFix\",\"FFD\",\"GPT\",\"TRT\",\"fixProp\"]\n",
        "mean_feat_values = {}\n",
        "print(\"Mean feature values in training data:\")\n",
        "for feat in features:\n",
        "    avg = np.mean(training_data[feat])\n",
        "    mean_feat_values[feat] = avg\n",
        "    print (feat, avg)\n",
        "\n",
        "# make dataframe of test data with mean values\n",
        "mean_data = test_data.copy()\n",
        "for feat in features:\n",
        "    mean_data[feat] = mean_feat_values[feat]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcb5c16-a29e-41e6-9b8b-f710d95335d9",
      "metadata": {
        "id": "5fcb5c16-a29e-41e6-9b8b-f710d95335d9"
      },
      "source": [
        "## Evaluation metric: Mean absolute error\n",
        "\n",
        "Now that we have the predictions of the baseline (the mean values) as well as the predictions of the pre-trained and fine-tuned models, we can compare them on an evaluation metric.\n",
        "\n",
        "We will evaluate prediction performance by using the _mean absolute error_ metric (MAE), meaning that we average over the error of all samples in the test set:\n",
        "\n",
        "$MAE = \\frac{\\sum_{i=1}^{n} | y_{i} - x_{i}|}{n} $\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e367fcb5-4009-4269-8532-ce812b62dcc4",
      "metadata": {
        "id": "e367fcb5-4009-4269-8532-ce812b62dcc4"
      },
      "outputs": [],
      "source": [
        "# calculate mean absolute error on the predictions\n",
        "print(\"Results pre-trained model:\")\n",
        "mae_pretrained = evaluate(predictions_pretrained, test_data)\n",
        "print(\"Results fine-tuned model:\")\n",
        "mae_finetuned = evaluate(predictions_finetuned, test_data)\n",
        "print(\"Results mean baseline:\")\n",
        "mae_mean = evaluate(mean_data, test_data)\n",
        "\n",
        "data = pd.DataFrame(data={'results': [\"mean\", \"pretrained\", \"finetuned\"], 'mae': [mae_mean, mae_pretrained, mae_finetuned]})\n",
        "ax = sns.barplot(x=\"results\", y=\"mae\", data=data, palette=\"flare\")\n",
        "ax.set_ylabel('MAE overall', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f464bd9-6574-471a-a9b4-84766bb29162",
      "metadata": {
        "id": "8f464bd9-6574-471a-a9b4-84766bb29162"
      },
      "source": [
        "<font color='limegreen'><b>Tip</b></font> If you are interested in seeing how good these results are compared to the systems participating in the CMCL Shared Task, have a look at Table 3 of [this paper](https://aclanthology.org/2021.cmcl-1.7.pdf).\n",
        "\n",
        "As the numbers and the plot above should show, the pre-trained model that is not trained on eye-tracking features but purely on text, does not beat the mean baseline. However, the model fine-tuned on eye-tracking data is substantially better (i.e., lower MAE) than the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gk22okYqDYrD",
      "metadata": {
        "cellView": "form",
        "id": "Gk22okYqDYrD"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 8</b></font> (1pt)\n",
        "#@markdown Is the difficulty of predicting individual eye-tracking features analogous in all models? Which feature is hardest to predict? Would you say the models are more capable of capturing early processing stages of lexical access or late-stage semantic integration? Which eye-tracking features illustrate this?\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D5_yFBLKEtA7",
      "metadata": {
        "cellView": "form",
        "id": "D5_yFBLKEtA7"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 9</b></font> (1pt)\n",
        "#@markdown Compare the error scores for eye-tracking features across models to the overview of the training data (in the _Training data overview_ section above). Why do you think the error for the features representing reading times (FFD, GPT, TRT) is lower than the error for the other features (nFix and fixProp)? **(0.5pt)** Why might the fixProp feature be particularly difficult to predict? **(0.5pt)**\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z1B-fr-pA5ML",
      "metadata": {
        "id": "z1B-fr-pA5ML"
      },
      "source": [
        "## Visualization of predictions\n",
        "We can also visualize the predictions of different models, the baseline and the true data for specific sentences.\n",
        "\n",
        "<font color='darkorange'>**Todo**</font> Compare the results for a few specific sentences and features (change the sentence_to_plot and feature_to_plot variables in the code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3b7094-8a72-472b-a5dd-9a20a7b0d560",
      "metadata": {
        "id": "3c3b7094-8a72-472b-a5dd-9a20a7b0d560"
      },
      "outputs": [],
      "source": [
        "# plot a sentence with real eye tracking data and the corresponding model predictions\n",
        "sentence_to_plot = 953 # 801, 901, 950, 953\n",
        "feature_to_plot = \"FFD\" # TRT, GPT, nFix, fixProp\n",
        "\n",
        "true_sentence = test_data.loc[test_data['sentence_id'] == sentence_to_plot]\n",
        "predicted_sentence_finetuned = predictions_finetuned.loc[predictions_finetuned['sentence_id'] == sentence_to_plot]\n",
        "predicted_sentence_pretrained = predictions_pretrained.loc[predictions_pretrained['sentence_id'] == sentence_to_plot]\n",
        "\n",
        "# todo: change fig size accroding to sent length\n",
        "fig, ax = plt.subplots(1,1,figsize=(len(true_sentence)*1.5,5))\n",
        "ax.plot(list(range(len(true_sentence))), true_sentence[feature_to_plot], label=\"correct\", color=\"blue\", linestyle=\"--\")\n",
        "ax.plot(list(range(len(predicted_sentence_finetuned))), predicted_sentence_finetuned[feature_to_plot], label=\"fine-tuned\", color=\"red\", marker='o', ms=10, linestyle=\"-.\")\n",
        "ax.plot(list(range(len(predicted_sentence_pretrained))), predicted_sentence_pretrained[feature_to_plot], label=\"pretrained\", color=\"pink\", marker='o', ms=10, linestyle=\"-.\")\n",
        "\n",
        "# todo: add mean as a task\n",
        "ax.plot(list(range(len(predicted_sentence_pretrained))), [mean_feat_values[feature_to_plot]]*len(predicted_sentence_pretrained), label=\"mean\", color=\"lightblue\", marker='o', ms=10, linestyle=\"--\")\n",
        "\n",
        "ax.set_ylabel(feature_to_plot, fontsize=16)\n",
        "ax.set_xticks(list(range(len(true_sentence))))\n",
        "ax.set_xticklabels(true_sentence['word'], fontsize=16)\n",
        "ax.legend(fontsize=13)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8232be-cbc7-4d59-a322-28ebd0e81de1",
      "metadata": {
        "id": "ea8232be-cbc7-4d59-a322-28ebd0e81de1"
      },
      "source": [
        "## Part of Speech Analysis\n",
        "\n",
        "Above, we have both evaluated the aggregated performance of the models as well as looked at the predictions for specific sentences.\n",
        "\n",
        "Next, we want to analyze whether there are certain word categories (i.e. parts of speech, PoS) for which eye tracking features are more easily predictable than others. Therefore, we first use a part-of-speech tagger to get the PoS tags for the sentence in our test data, and then plot the predictions vs. true eye tracking data per PoS tag below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0467fb-1675-4cd9-b479-c4ec929490ad",
      "metadata": {
        "id": "2f0467fb-1675-4cd9-b479-c4ec929490ad"
      },
      "outputs": [],
      "source": [
        "# tag the data and add the tags to the dataframe\n",
        "def pos_tag(data_to_tag):\n",
        "    words = [str(w).replace(\"<EOS>\", \".\") for w in data_to_tag['word']]\n",
        "    tagged_sents = nlp(custom_tokenizer(words))\n",
        "    data_to_tag['pos_tags'] = [token.pos_ for token in tagged_sents]\n",
        "    return data_to_tag\n",
        "\n",
        "tagged_data = pos_tag(test_data)\n",
        "tagged_predictions = pos_tag(predictions_finetuned)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O_Df-H6qMjmc",
      "metadata": {
        "id": "O_Df-H6qMjmc"
      },
      "source": [
        "<font color='darkorange'>**Todo**</font> Compare the results for a few different features (change the feature_to_plot variable in the code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19a9717-144f-4245-a43a-6adff6926bf1",
      "metadata": {
        "id": "c19a9717-144f-4245-a43a-6adff6926bf1"
      },
      "outputs": [],
      "source": [
        "feature_to_plot = \"TRT\"\n",
        "\n",
        "#fig = plt.figure(figsize=(10,5))\n",
        "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
        "sns.barplot(x=\"pos_tags\", y=feature_to_plot, data=tagged_data, color=\"green\", label=\"true eye tracking data\", alpha=0.4)\n",
        "sns.barplot(x=\"pos_tags\", y=feature_to_plot, data=tagged_predictions, color=\"red\", label=\"predicted\", alpha=0.4)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sl3yGs7uI7yA",
      "metadata": {
        "id": "sl3yGs7uI7yA"
      },
      "source": [
        "<font color='limegreen'><b>Tip</b></font> To look up what the PoS tag abbreviations mean, you can use the `spacy.explain()` function as demonstrated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0uZdIa7HI4wr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0uZdIa7HI4wr",
        "outputId": "847674b6-3106-4326-d032-30a42e739d31"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'adjective'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.explain('ADJ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UuPrbqDVJikK",
      "metadata": {
        "cellView": "form",
        "id": "UuPrbqDVJikK"
      },
      "outputs": [],
      "source": [
        "#@title <font color='cornflowerblue'><b>ToThink 10</b></font> (1pt)\n",
        "#@markdown Can you think of any additional linguistic analyses that would be interesting to compare the models' performances on?\n",
        "Response = \"\" #@param {type:\"string\"}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit ('3.7.6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e9865b8fc30c7210bbbe2c0b0464dbc9700000eff1f4e229588b0a8358506f7f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}